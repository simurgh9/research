\documentclass{homework}
\input{lib/preamble}
\input{lib/macro}
% \darkmode

\begin{document}
\begin{abstract}
  \blindtext
\end{abstract}
\maketitle

\section{Introduction}\label{intro}

A lattice is a discrete additive subgroup of a $d$-dimensional
Euclidean space. Intuitively, lattice points are to euclidean spaces
what multiples of $d$ are to the real number line. Consider a basis
matrix $\B \in \R^{d \times d}$ with $d$ linearly independent column
vectors $\{1 \leq i \leq d : b_i \in \R^d\}$ such that $b_i$ is the
$i^\text{th}$ column of $\B$. We recall $\R^d$ as,
\[
  \R^d = \{x \in \R^d : \B x\}
\]
If we restrict $x\in \Z^d$ \ie $\B x$ to only the integral linear
combinations, we obtain a lattice,
\[
  \L = \{x \in \Z^d : \B x\}
\]
You may also see it more commonly (see
\cite{micciancio2002complexity}) written as,
\begin{align*}
  \L(b_1,\dots,b_d)
         & = \curl{x_i \in \Z : x_1b_1 + x_2b_2 + x_3b_3 + \cdots + x_db_d} \\
  \L(\B) & = \curl{x_i \in \Z : \sum_{i=1}^{d}x_ib_i}
\end{align*}
The dimension of the lattice is $\dim(\L) = d$ \ie the number of
column-vectors in the basis matrix $\B$. And--since $\B$ also has $d$
number of rows, we call $\L$ a full rank lattice. $\R^d$ is now the
ambient space for the reduced set of vectors that are in the lattice
$\L$. The group operation of a lattice is the vector difference,
\[
  u \in \L \ni v,\quad (v - u) \in \L
\]

If we further restrict $\B \in \Z^{d \times d}$, the resulting lattice
is known as an integral lattice. We show the plot of a two dimensional
lattice ($d=2$) spanned by the following basis in figure
\ref{2dlattice}.
\[
  \B_\text{bad} =
  \begin{bmatrix}
    95  & 47  \\
    460 & 215
  \end{bmatrix},
  \quad
  \B_\text{good} =
  \begin{bmatrix}
    1  & 40 \\
    30 & 5
  \end{bmatrix}
\]
You might imagine another trivial lattice $\Z^2$ spanned by the
identity matrix.

\img<2dlattice>[0.6]{An example two dimensional lattice.}{2dlattice}

In this paper, we will also discuss the lattices formed over the
Gaussian integers. Despite the name, these are complex numbers where
the real and the imaginary parts are limited to the integers.
\[
  \G = \{(a,b)\in \Z^2: a + bi\}, \quad i^2 = -1
\]
We can similarly form a lattice, this time in the ambient space of
complex numbers $\C$, by letting $\B \in \G^{d \times d}$ such that,
\[
  \L = \{x \in \G^d : \B x\}
\]


\subsection{Lattice Problems}

Under the restrictions of randomised reduction hypothesis
\cite{ajtai1998shortest} both of the problems given bellow (\SVP,
\CVP) are \NP-hard. Let $||v||$ be the Euclidean norm of a vector $v$
then,

\subsubsection{Shortest Vector Problem (\SVP)} Find the shortest
non-zero vector $v_i \in \L$ such that for all $v_j \in \L$ where
$i \neq j$,
\[
  \norm{v_i} < \norm{v_j}
\]
An easier optimisation version of the \SVP{} is stated as follows,
\subsubsection{Approximate Shortest Vector Problem (\appr\SVP)} Let
$\alpha \geq 1$ be an approximation factor and find a $v \in \L$ such
that $\norm{v}$ is no bigger than $\alpha$ times the length of the
shortest vector.
\[
  \norm{v} \leq \alpha\norm{v_\text{shortest}}
\]
\subsubsection{Closest Vector Problem (\CVP)} For $w \in \R^d$ where
$w \not\in \L$ find a vector $v \in \L$ that is closest to $w$.
\[
  \min_{v \in \L} \norm{w - v}\quad\text{where}\quad \norm{w - v} > 0
\]

\subsubsection{Shortest Basis Problem (\SBP)} Given a bad basis
$\B_\text{bad}$, reduce it to a good basis $\B_\text{good}$. The
goodness maybe achieved by minimising the lengths of the basis vectors
\eg
\[
  \sum_{i=1}^{d}\norm{b_i}^2 = \norm{b_1}^2 + \norm{b_2}^2 + \norm{b_3}^2
  + \cdots + \norm{b_d}^2
\]
while optionally also requiring $\mathcal{H}(\B_\text{good})$ to be
closer to 1. Figure \ref{2dlattice} shows two basis that span the same
lattice. One is labelled as ``good'' and the other as ``bad.'' A basis
gets better as it's vectors get shorter and more orthogonal. This
measure of orthogonality is expressed as the \textit{Hadamard ratio}
of a basis $\B$,
\[
  \mathcal{H}(\B) = \paren{
    \frac{\det \L}{\norm{b_1}\norm{b_2}\norm{b_3}\cdots \norm{b_d}}
  }^\frac{1}{d}
\]
Note that $0 < \mathcal{H}(\B) < 1$ and $\det \L = |\det \B|$. The
closer $\mathcal{H}(\B)$ to 1, the better the basis $\B$.

The process of turning a bad basis into a good basis is also sometimes
referred to as performing lattice reduction. I. e., \SBP{} maybe
solved via the various lattice reduction algorithms. As discussed in
the section \ref{rw}, any solution to the \SBP{} often also uncovers a
comparable solution to the \appr\SVP. Essentially all
lattice-cryptography (popular choice for quantum-safe cryptosystems)
relay on the inability of lattice reduction algorithms to solve the
\appr\SVP{} with an approximation of $\alpha \in \O(\sqrt{d})$.

\subsection{Gaussian Heuristic} It is difficult to verify a solution
for the \appr\SVP{} and \SVP{} since the length of the shortest vector
is unknown in the general case. As the dimension $d$ of a lattice
increases, we may relay on the Gaussian expected shortest length, also
known as the Gaussian heuristic (see \cite{hoffstein2008introduction},
\cite{siegel1945mean} for more justification). For a small $\ep > 0$
and a sufficiently large $d$, the shortest vector in a random lattice
satisfies,
\[
  (1-\ep)\sigma(\L) \leq
  \norm{v_\text{shortest}}
  \leq (1+\ep)\sigma(\L)
\]
whereas,
\[
  \sigma(\L) = \sqrt{\frac{d}{2\pi e}}(\det \L)^\frac{1}{d}
  \quad
  \text{or}
  \quad
  \norm{v_\text{shortest}} \approx \sigma(\L)
\]
\section{Related Work}\label{rw}

If any of the \NP-complete problems are shown to be in \P{} then all
of \NP-problems are in \P. In the same seminal paper
\cite{karp1972reducibility} where Karp determines a subset of the
\NP{} problems as \NP-complete with the aforementioned property, he
also gives twenty-one examples of such \NP-complete problems. Number
18 on the list is the knapsack problem: Given a set
$M \in \N^n, S \in \N$ find $x \in \curl{0, 1}^n$ such that $Mx =
  S$. In other words, find a subset of $M$ whose sum is equal to $S$.

The first cryptosystem to be based on an \NP-complete problem uses a
disguised knapsack problem \cite{hoffstein2008introduction} and was
attempted by Merkle and Hellman \cite{merkle2019hiding}. Note that we
say a \textit{disguised} knapsack problem since whether a
cryptographic system can be as hard to break as an \NP-complete
problem is an open problem in itself \cite{pass2006parallel}.

Lagarias and Odlyzko \cite{lagarias1985solving} showed that any
knapsack problem can be encoded as an \SVP. We state the gist of their
idea. Take any knapsack problem $M = \{r_1, r_2, \hdots, r_n\}$ with
$S$ and the relevant solution $x$ such that $Mx = S$. Now consider the
following lattice basis in $\N^{d \times d}$ with dimension
$d = n + 1$,
\[
  \B_\text{bad} =
  \begin{bmatrix}
    2      & 0      & 0      & \cdots & 0      & 1      \\
    0      & 2      & 0      & \cdots & 0      & 1      \\
    0      & 0      & 2      & \cdots & 0      & 1      \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0      & 0      & 0      & \cdots & 2      & 1      \\
    r_1    & r_2    & r_3    & \cdots & r_n    & S      \\
  \end{bmatrix}
\]
The lattice spanned by $\B_\text{bad}$ must have a vector $t$ that is
the result of an integral linear combination due to $x$,
\[
  t =
  \begin{bmatrix}
    2      & 0      & 0      & \cdots & 0      & 1      \\
    0      & 2      & 0      & \cdots & 0      & 1      \\
    0      & 0      & 2      & \cdots & 0      & 1      \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0      & 0      & 0      & \cdots & 2      & 1      \\
    r_1    & r_2    & r_3    & \cdots & r_n    & S      \\
  \end{bmatrix}
  \col{x_1, x_2, x_3, \vdots, x_n, -1} =
  \col{2x_1-1, 2x_2-1, 2x_3-1, \vdots, 2x_n-1, M\cdot x-S} =
  \col{2x_1-1, 2x_2-1, 2x_3-1, \vdots, 2x_n-1, 0}
\]
Since $x \in \{0, 1\}^n$ then any $(2x_i-1) \in t$ must be either $1$
or $-1$. Therefore,
\[
  \norm{t} = \sqrt{n}
\]
$t$ is very likely to be the shortest vector in
$\L(\B_\text{bad})$. The shortest vector in the lattice spanned by
$\B_\text{bad}$ will reveal the solution $x$ to the knapsack problem.

\subsection{Lattice Reduction Algorithms} If a lattice is expressed in
terms of it's good basis then solving the \SVP{} becomes fairly
easy. This means that if we first solve the \SBP{} then the \SVP{} is
easy. For example, assume for a certain $\B_\text{good}$ that all
column vectors $b_i$ are pairwise orthogonal \ie for $i \neq j$ we
know that $b_i\cdot b_j = 0$. Then for any $x \in \Z^d$,
\[
  \norm{x_1b_1 + x_2b_2 + x_3b_3 + \cdots x_db_d}^2 =
  x_1^2\norm{b_1}^2 + x_2^2\norm{b_2}^2 + x_3^2\norm{b_3}^2 + \cdots
  x_d^2\norm{b_d}^2
\]
and the shortest non-zero vector(s) can be found in,
\[
  \curl{\pm b_1, \pm b_2, \pm b_3, \cdots \pm b_d}
\]

Similarly for an approximate solution of an instance of the \CVP{}
using a good basis, see the Babai's nearest hyperplane algorithm
\cite{babai1986lovasz} or Theorem 7.34 (pg. 405) of Hoffstein
\cite{hoffstein2008introduction}.

\subsubsection{Lenstra, Lenstra, and Lov\'asz (\LLL) Algorithm} The
first lattice reduction algorithm is by Gauss. It works like the
Euclid's greatest common divisor (highest common factor) algorithm but
with two, two-dimensional vectors \ie $\B = \{b_1, b_2\}$. Assume
without the loss of generality that $\norm{b_1} < \norm{b_2}$ then,
\[
  b_2 = b_2 - \near{\frac{b_1 \cdot b_2}{b_1 \cdot b_1}}b_1
\]
If $\norm{b_2}$ is still greater than $\norm{b_1}$ we can
stop. Otherwise, swap $b_2$ with $b_1$ and try again.

The \LLL{} algorithm generalises the Gaussian lattice reduction from
two to $d$ dimensions. Just like the Gaussian lattice reduction, it
subtracts an integral multiple of a shorter basis vector from a larger
basis vector till some size condition is fulfilled. For each
$1 \leq k \leq d$, we reduce $b_k$ as the following,
\[
  b_k = b_k - \near{\frac{b_j^* \cdot b_k}{b_j^* \cdot b_j^*}}b_j
\]
$b_j^*$ is the $j^\text{th}$ basis vector in the Gram-Schmidt
orthogonalization of $\B$. The reduction of $b_k$ is carried out using
the Gram-Schmidt orthogonalizations of all the already reduced $b_j$
for every $k - 1 > j \geq 1$ where the following \textit{size
  condition} is met,
\[
  \abs{\frac{b_j^* \cdot b_k}{b_j^* \cdot b_j^*}} > \frac{1}{2}
\]
If \LLL{} terminates after only this recursive size reduction then the
goodness of the reduced basis depends on the order of the original
basis vectors in the basis matrix. Therefore, after the size reduction
of $b_k$, another condition, namely the popular Lov\'asz condition is
checked. This is given as,
\[
  \norm{b_k^*}^2 \geq \paren{
    \frac{3}{4} - \paren{
      \frac{b_{k-1}^* \cdot b_k}{b_{k-1}^* \cdot b_{k-1}^*}
    }^2
  }
  \norm{b_{k-1}^*}^2
\]
Written another way for $\mu_{k, k-1} =
\paren{b_{k-1}^* \cdot b_k}/
\paren{b_{k-1}^* \cdot b_{k-1}^*}$ and $\delta = 3/4$,

\[
  \norm{b_k^*}^2 \geq \paren{\delta - \mu_{k, k-1}^2}\norm{b_{k-1}^*}^2
\]
If the Lov\'asz condition is met then $b_k$ is considered reduced and
$k$ will be incremented. Otherwise, for optimal ordering, we swap
$b_k$ and $b_{k-1}$ and decrement $k$\footnote{More precisely
  $k = \max(k - 1, 2)$}. For a more in depth analysis of \LLL{}, read
Deng \cite{deng2016introduction}. The full description is given in
algorithm \ref{lllalg}.

\begin{algorithm}
  \begin{flushleft}
    \noindent\textbf{Input}: Lov\'asz condition constant: $0 < \delta < 1$. \\
    \noindent\textbf{Input}: Bad basis: $\B = \{b_1, b_2, b_3, \dots, b_d\}$. \\
    \noindent\textbf{Output}: Good/Reduced basis:
    $\B = \{b_1, b_2, b_3, \dots, b_d\}$.
  \end{flushleft}
  \begin{enumerate}[label=\arabic*:]
    \item $k \leftarrow 2$
    \item $(\B^*, \mu) \leftarrow \GM(\B)$
    \item $\quad \WHILE k \leq d$
    \item $\quad\quad \FOR j \in \{k - 1, k - 2, k - 3, \dots, 1\}$
    \item $\quad\quad\quad \IF \mu_{k, j} > 0.5$
    \item $\quad\quad\quad\quad b_k \leftarrow b_k - \near{\mu_{k, j}}b_j$
    \item $\quad\quad\quad\quad (\B^*, \mu) \leftarrow \GM(\B)$
    \item $\quad\quad \IF \norm{b_k^*}^2 \geq
            \paren{\delta - \mu_{k, k-1}^2}\norm{b_{k-1}^*}^2$
    \item $\quad\quad\quad k \leftarrow k + 1$
    \item $\quad\quad \ELSE$
    \item $\quad\quad\quad \SWAP(b_{k-1}, b_k)$
    \item $\quad\quad\quad (\B^*, \mu) \leftarrow \GM(\B)$
    \item $\quad\quad\quad k \leftarrow \max(k - 1, 2)$
  \end{enumerate}
  \caption{The Lenstra, Lenstra, and Lov\'asz (\LLL) algorithm.}
  \label{lllalg}
\end{algorithm}

The authors \cite{lenstra1982factoring} of the \LLL{} algorithm show
that it is a polynomial time lattice reduction algorithm for all $0 <
\delta < 1$. The outer loop runs at most in,
\[
  \O(d^2\log(d) + d^2\log(\max\norm{b_i}))
\]
and that it solves the \appr\SVP{} with an approximation factor of
$\alpha = 2^{(d-1)/2}$. It is also an open problem whether \LLL{}
terminates in polynomial time for $\delta = 1.$

\subsubsection{\LLL{} Variations} While \LLL{} is a polynomial time
algorithm for lattice reduction, many of it's generalisations tend to
perform just as fast during empirical analysis and yield a further
reduced basis. Gamma et alia argue by their extensive empirical
analysis \cite{gama2008predicting} that there is a gap between what
theory is able to prove and what the true power of the reduction
algorithms maybe.

The two possible exponential time variations of \LLL{} are due to
Schnorr \cite{schnorr1987hierarchy} and Euchner
\cite{schnorr1994lattice}: DEEP (deep insertion method) and BKZ (block
Korkin--Zolotarev). DEEP differs from the standard \LLL{} when the
Lov\'asz condition fails. In standard \LLL{} we simply swap the $b_k$
with $b_{k-1}$ whereas in DEEP we insert $b_k$ at an optimal place
before the $k^\text{th}$ basis vector. While in BKZ, instead of
reducing $b_k$ with only one $b_{j}$, the same is done with a block of
basis vectors, $b_j, b_{j+1}, b_{j+2}, \dots b_{j+\beta - 1}$ where
$\beta$ is the block size. Note that if we let $\beta=d$ then the
shortest vector in the output of BKZ solves the \SVP{} problem and for
any $\beta < d$ we solve some version of the \appr\SVP{}.

\section{Methodology}

Apart from the reduction algorithms mentioned in section \ref{rw}, we
now discuss the approach taken in this paper. The method of sieving
for the shortest vector first surfaced due to the works of Ajtai,
Kumar and Sivakumar \cite{ajtai2001sieve}. Today, sieving algorithms
have become very practical. At the time of this writing, more than
half ($437/847$) of the solutions posted at the \textit{Shortest
  Vector Problem Challenges} website \cite{LatticeChallenge2025} are
done via some sieving technique.

\subsection{Sieving} The idea behind sieving is quite simple: if we
have two lattice vectors $u \in \L \ni v$ and we want a shorter one,
we might try $v - u$. More specifically, repeatedly check if there
exists a pair of vectors $(u, v)$ in a fixed subset $P$ of $\L$ such
that $\norm{v - u} < \norm{v}$ then $v$ is replaced with $v - u$. When
$\norm{v - u} < \norm{v}$ is not true for any pair $(u, v)$ in $P^2$,
we hope to solve some version of the \appr\SVP{} with the shortest
vector in $P$. We give our variation of the aforementioned idea as a
na\"ive sieving algorithm \ref{naive}.

\begin{algorithm}
\begin{flushleft}
    \noindent\textbf{Input}: $P \subset \L$. \\
    \noindent\textbf{Output}: Reduced version of subset: $P.$
  \end{flushleft}
  \begin{enumerate}[label=\arabic*:]
    \item $\DO$
    \item $\quad R \leftarrow \nil$
    \item $\quad \FOR \text{each } (u, v) \in P^2$
    \item $\quad\quad t \leftarrow v - u$
    \item $\quad\quad \IF (\vec{0} \neq t \not\in P) \wedge
    (\norm{t} < \norm{u} \vee \norm{t} < \norm{v})$
    \item $\quad\quad\quad R \leftarrow R \cup \{t\}$
    \item $\quad P \leftarrow \SL(P \cup R)$
    \item $\WHILE R \neq \nil$
  \end{enumerate}
  \caption{Na\"ive sieving algorithm.}
  \label{naive}
\end{algorithm}

Algorithm \ref{naive} checks the difference of all the possible
pairings in $P$ for a shorter non-zero vector not already present in
$P$. These newer and shorter vectors are collected in $R$. At the
start of each iteration, we combine $P$ and $R$, \textit{selecting}
out of the combination at most $|P|$ successive shortest vectors to be
reassigned as $P$. The sieving terminates when $P^2$ no longer
contains a pair $(u, v)$ whose difference's length is shorter than any
of the ones already in $P$.

We show an example by reducing the bad basis given in section
\ref{intro},
\[
  \B_\text{bad} =
  \begin{bmatrix}
    95  & 47  \\
    460 & 215
  \end{bmatrix}
\]
The initial $P$ is randomly picked as follows,
\[
  P =
  \begin{bmatrix}
    46  & 94  & 97  & 475 \\
    185 & 430 & 520 & 2300
  \end{bmatrix}
\]
and figure \ref{naivedemo} shows the new $P$ at each iteration of a
total of seven iterations that were ran.

\img<naivedemo>[0.32]{Algorithm \ref{naive} reducing
$\B_\text{bad}$ with
$P =
  \begin{bmatrix}
    46  & 94  & 97  & 475 \\
    185 & 430 & 520 & 2300
  \end{bmatrix}
$.}{naive1, naive2, naive3, naive4, naive5, naive6, naive7}

Note that in this case, we find the shortest vector by solving the
\SBP{} exactly. The seventh iteration in figure \ref{naivedemo}
contains,
\[
  \B_\text{good} =
  \begin{bmatrix}
    -1  & 40 \\
    -30 & 5
  \end{bmatrix}
\]

\subsection{Genetic Algorithms} Genetic algorithms simulate the theory
of biological evolution and natural selection to solve a range of
mathematical-optimisation problems. Just like in naturally occurring
evolution where individuals in a population evolve to gain desirable
features (or, rather individuals with undesirable features go
extinct), machine evolution starts with an initial population and
hopes for individuals to be optimised after a certain number of
generations of the same population.

To solve an optimisation problem with a genetic algorithm, we start by
designing a \textit{schema} to encode a possible solution. An instance
of this schema is referred to as an \textit{individual}. A set of such
individuals is a \textit{population}. Population of individuals evolve
through \textit{generations} by the means of reproduction. Using a
\textit{selection} process based on a \textit{fitness-function}, two
parent individuals are selected for \textit{crossover} to birth a
child individual in the next generation. The child individual may go
through a \textit{mutation}. We describe a general genetic
optimisation in algorithm \ref{ga}.

\begin{algorithm}
  \begin{flushleft}
    \noindent\textbf{Input}:
      Initial population: $P$, fitness-function: $\FIT: P \ra [0, 1]$. \\
    \noindent\textbf{Output}:
      Optimal $P$ as the last generation.
  \end{flushleft}
  \begin{enumerate}[label=\arabic*:]
    \item $i \leftarrow 1$
    \item $\DO$
    \item $\quad R \leftarrow \nil$
    \item $\quad \FOR (u, v) \leftarrow \SL(P, \FIT)$
    \item $\quad\quad t \leftarrow \CROSS(u, v)$
    \item $\quad\quad \IF (\text{small random probability})$
    \item $\quad\quad\quad t \leftarrow \MUTATE(t)$
    \item $\quad\quad R \leftarrow R \cup \{t\}$
    \item $\quad P \leftarrow R$
    \item $\quad i \leftarrow i + 1$
    \item $\WHILE (\forall v \in P, \FIT(v) \geq \ep_1) \vee (i < \ep_2)$
  \end{enumerate}
  \caption{Generic genetic algorithm from Norvig (pg. 129)
    \cite{russell2016artificial}.}
  \label{ga}
\end{algorithm}

Algorithm \ref{ga} has a potential pitfall: it may lose the best
individuals from one generation to another during crossover or
mutation due to the probabilistic selection. This can cause divergence
instead of convergence to a local optimum. To mitigate this and
similar issues, we will employ \textit{elitism}
\cite{de1975analysis}. Elitism ensures that the $|P|$ fittest
individuals are carried over to the next generation from the pool
$P \cup R$. As a result, the next generation will consist of the best
individuals outside of the parents and offsprings set.


\subsection{Genetic Algorithm for Sieving} As also observed by
Laarhoven \cite{DBLP:journals/corr/abs-1907-04629} sieving for the
shortest vector can be naturally expressed as a genetic
algorithm. Following is a specification for each of the components of
the genetic algorithm used in this paper to solve the \appr\SVP. The
whole algorithm is given in algorithm \ref{gas}.

\subsubsection{Schema} We represent vectors $v \in \L$ as is,
\[
  v = \arr{v_1, v_2, v_3, \dots, v_d}
\]
Note that here $\L \subset \Z^{d\times d}$ or $\L \subset \G^{d\times d}$.

\subsubsection{Initial Population} Before we proceed with the
generation of the initial population using $\B$, we may
\textit{optionally} compute the \textit{Hermite normal form} of $\B$
as well as reduce it using the \LLL{} algorithm for some $\delta$. The
Hermite normal form is to integral matrices what the reduced echelon
form is for the matrices over the reals.

Let $n = |P|$, we generate the initial population via a constant $d$
by $n$ matrix like so,
\[
  P = \B C
\]
If $\L(\B) \subset \Z^{d\times d}$ or $\B \in \Z^{d\times d}$,
\[
  C \in \{0, 1\}^{d\times n}\quad\text{where}\quad
  \text{Pr}(C_{i,j} = 1) = \rho
\]
However, if $\L(\B) \subset \G^{d\times d}$ or $\B \in \G^{d\times d}$
then for $a \in \{0, 1\} \ni b$,
\[
  C_{i,j} = a + bi\quad\text{where}\quad
  \text{Pr}(a = 1) = \text{Pr}(b = 1) = \rho
\]

\subsubsection{Selection Strategy} Let $P$ be the previous generation
and $R$ be the current generation, or the generation produced as a
result of the reproduction among $P$. The next generation is then
produced by picking the $|P|$ successive shortest vectors from the
pool $P \cup R$ and then assigning them back to $P$. Observe how this
implies that $P$ will now be sorted by vector length in ascending
order.
\[
  P \leftarrow \Elite(P \cup R)
\]
We will use this $P$ to select the individuals for crossover. Let
$u_i, v_j$ be the $i^\text{th}$ and $j^\text{th}$ vectors in $P$ then
algorithm \ref{sl} states how we can generate pairings of vectors in
$P$. Note that at most algorithm \ref{sl} generates $^{|P|}C_2$ pairs.

\begin{algorithm}
  \begin{flushleft}
    \noindent\textbf{Input}: Population: $P$. \\
    \noindent\textbf{Output}: Generated pairs for reproduction.
  \end{flushleft}
  \begin{enumerate}[label=\arabic*:]
    \item $\FOR i \in \{1, 2, 3, \dots |P| - 1\}$
    \item $\quad \FOR j \in \{i + 1, \dots |P|\}$
    \item $\quad\quad \YIELD (u_i, v_j)$
  \end{enumerate}
  \caption{Pair $(u_i, v_j)$ generator of $P$ for crossover.}
  \label{sl}
\end{algorithm}

\subsubsection{Fitness Function} We evaluate the fitness of a vector
by the inverse of its $\ell^2$ norm.
\[
  \FIT(v) = \frac{1}{\norm{v}_2} = \frac{1}{\sqrt{v^Tv}}
\]

If $v \in \G^d$ then,
\[
  \norm{v}_2^2 = v^Hv = |v_1|^2 + |v_2|^2 +  |v_2|^2 + \cdots + |v_d|^2
\]
where $|v_j|^2 = a^2 + b^2$ for $v_j = a + bi$. The $v^H$ is known as
the conjugate transpose of $v$.

\subsubsection{Crossover} For $u \in P \ni v$,
\[
  t = v - \near{\mu} u
\]
Note that for $z = a + bi$ where $(a, b) \in \R^2$, we define:
$\near{z} = \near{a} + \near{b}i$.

If $u \in \Z^d \ni v$,
\[
  \mu = \frac{u\cdot v}{u\cdot u}
\]
Or if $u \in \G^d \ni v$,
\[
  \mu = \frac{\Re(u\cdot v)}{u\cdot u} + \frac{\Im(u\cdot v)}{u\cdot u}i
\]

\subsubsection{Mutation} The $j^\text{th}$ column of the initial
population (and the subsequent generations) is given as,
\[
  P_j =
  \begin{bmatrix}
    b_{1,1}c_{1,j} + b_{1,2}c_{2,j} + \cdots  b_{1,d}c_{d,j} \\
    b_{2,1}c_{1,j} + b_{2,2}c_{2,j} + \cdots  b_{2,d}c_{d,j} \\
    \vdots \\
    b_{d,1}c_{1,j} + b_{d,2}c_{2,j} + \cdots  b_{d,d}c_{d,j}
  \end{bmatrix}
  =
  \begin{bmatrix}
    b_{1,1} & b_{1,2} & \cdots  b_{1,d} \\
    b_{2,1} & b_{2,2} & \cdots  b_{2,d} \\
            & \vdots  &                 \\
    b_{d,1} & b_{d,2} & \cdots  b_{d,d}
  \end{bmatrix}
  \begin{bmatrix}
    c_{1,j} \\
    c_{2,j} \\
    \vdots \\
    c_{d,j}
  \end{bmatrix}
  =
  \B c_j.
\]
Laarhoven \cite{DBLP:journals/corr/abs-1907-04629} represent each
individual in the population as $c_j$ instead of $\B c_j$. This
enabled them to mutate $\B c_j$ by adding a small integral
perturbation to $c_{i,j}$. E. g.,
\[
  c_{i,j} = c_{i,j}+1 \mod 2
\]
Another possible approach is,
\[
  \ep \sim \mathcal{N}(\vec{0}, \mathbb{I}_d), \quad P_j = \B\near{c_j + \ep}
\]

However, mutations generally decrease the fitness; if at all useful
\cite{DBLP:journals/corr/abs-1907-04629}. While mutating is an
interesting question for future work, we do not mutate individual
vectors in this paper.

\begin{algorithm}
  \begin{flushleft}
    \noindent\textbf{Input}:
    \begin{enumerate}
      \item Basis: $\B$
      \item Population size: $n$
      \item Sampling density: $\rho$
      \item Reproduction cut-off: $\eta$
    \end{enumerate}
    \noindent\textbf{Output}: Approximation of the shortest vector.
  \end{flushleft}
  \begin{enumerate}[label=\arabic*:]
    \item $C_{d,n} \leftarrow (c_{i,j} \sim \text{Bernoulli}(\rho))$
    \item $P \leftarrow \B C$
    \item $R \leftarrow \nil$
    \item $\DO$
    \item $\quad P \leftarrow \Elite(P \cup R)$
    \item $\quad R \leftarrow \nil$
    \item $\quad \DO$
    \item $\quad\quad u, v \leftarrow \SL(P)$
    \item $\quad\quad t \leftarrow \CROSS(u, v)$
    \item $\quad\quad \IF (\vec{0} \neq t \not\in P) \wedge
    (\norm{t} < \norm{u} \vee \norm{t} < \norm{v})$
    \item $\quad\quad\quad R \leftarrow R \cup \{t\}$
    \item $\quad \WHILE |R| < \eta \binom{n}{2}$
    \item $\WHILE \forall v \in P, \FIT(v) \geq \ep$
  \end{enumerate}
  \caption{Genetic sieving algorithm.}
  \label{gas}
\end{algorithm}

% \section{Results}
% \section{Conclusion}
% \section{Acknowledgement}

% ---
\newpage
\bibliographystyle{unsrt}
\bibliography{citation}
\end{document}

% LocalWords:  Merkle Lagarias Odlyzko Hoffstein offsprings hermite
% LocalWords:  LLL Ajtai Sivakumar
